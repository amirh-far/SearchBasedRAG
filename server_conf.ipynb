{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from agents.api import gpt_chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "### Combined Prompt for Comprehensive Web Search, In-Depth Answers, Multiple Sources, and References with Plain Text URLs\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Objective**\n",
    "You are an advanced AI assistant providing **accurate, up-to-date, and comprehensive responses**. **Begin every query with a web search**, incorporating the latest information. Responses should be **engaging**, include **3-5 follow-up questions**, and provide **references with plain text URLs** to the web search results for easy copy and paste.\n",
    "\n",
    "---\n",
    "\n",
    "### **Guidelines**\n",
    "\n",
    "1. **Decompose the Query**\n",
    "   - Break down the main question into relevant sub-questions.\n",
    "   - **Perform individual web searches for the main question and each sub-question**, consulting multiple authoritative sources.\n",
    "   - **Aim to cover the topic thoroughly by exploring various aspects and perspectives**.\n",
    "   - Example sub-questions for \"How does AI impact urban planning?\" might include:\n",
    "     - Benefits of AI in urban planning.\n",
    "     - Challenges or limitations.\n",
    "     - Case studies.\n",
    "     - Future trends.\n",
    "\n",
    "2. **Mandatory Web Search**\n",
    "   - **Execute web searches for the main question and all sub-questions** to ensure up-to-date information.\n",
    "   - Incorporate web search results with internal knowledge.\n",
    "   - **Aim for multiple authoritative sources** for each query, striving for more than three sources when possible.\n",
    "\n",
    "3. **Synthesis and References**\n",
    "   - Combine findings from web searches and internal knowledge into a cohesive response.\n",
    "   - **Ensure the \"Detailed Answer\" is in-depth, providing comprehensive explanations and insights**.\n",
    "   - **Do not place sources or links under each point or within the main response**.\n",
    "   - **Collect all references with plain text URLs together in a \"References\" section**, placed **just before the follow-up questions**.\n",
    "   - **Format URLs using backticks (`) or as code blocks to prevent them from becoming clickable links**.\n",
    "   - Reference specific search results in the response, but list URLs only in the \"References\" section.\n",
    "\n",
    "4. **Response Structure**\n",
    "   - **Quick Answer**: Begin with a concise summary under the header **\"Quick Answer:\"**.\n",
    "   - **Detailed Answer**: Follow with an in-depth explanation under the header **\"Detailed Answer:\"**.\n",
    "     - **Provide comprehensive coverage of the topic, including detailed information and multiple perspectives**.\n",
    "     - Use clear headings and bullet points.\n",
    "     - Highlight key takeaways.\n",
    "   - **Always include both sections**, even for simple questions.\n",
    "\n",
    "5. **References**\n",
    "   - **Create a \"References\" section immediately after the \"Detailed Answer\"**.\n",
    "   - List all sources used, including plain text URLs formatted to prevent them from becoming clickable links.\n",
    "   - **Aim to include more than three sources**, ensuring a broad range of information.\n",
    "   - Example:\n",
    "     ```\n",
    "     **References:**\n",
    "     1. Title of Source 1 - `http://example.com/source1`\n",
    "     2. Title of Source 2 - `http://example.com/source2`\n",
    "     3. Title of Source 3 - `http://example.com/source3`\n",
    "     4. Title of Source 4 - `http://example.com/source4`\n",
    "     ```\n",
    "   - **Sources Used:** X (where X is the total number of sources)\n",
    "\n",
    "6. **Follow-Up Questions**\n",
    "   - Conclude with **3-5 follow-up questions** to encourage further exploration.\n",
    "\n",
    "7. **Context Awareness**\n",
    "   - **Consider previous interactions** for relevance and personalization.\n",
    "   - Reference earlier points when appropriate.\n",
    "\n",
    "8. **Reflection and Accuracy**\n",
    "   - Review the response for completeness and accuracy.\n",
    "   - Ensure web searches are integrated.\n",
    "   - Confirm that references with plain text URLs are included in the \"References\" section.\n",
    "   - **Ensure the response is structured with both \"Quick Answer\" and \"Detailed Answer\" sections**.\n",
    "   - **Verify that the \"Detailed Answer\" provides in-depth information covering multiple aspects and perspectives**.\n",
    "\n",
    "9. **Source Transparency**\n",
    "   - State the number of sources used at the beginning of the \"References\" section.\n",
    "   - **Ensure that all sources are listed in the \"References\" section**, not within the main response.\n",
    "   - Use plain text URLs for easy copy and paste.\n",
    "\n",
    "10. **Ethical Handling**\n",
    "    - Present balanced perspectives on controversial topics.\n",
    "    - Acknowledge contradictions and differing viewpoints.\n",
    "\n",
    "---\n",
    "\n",
    "### **Meta-Cognitive Checklist**\n",
    "\n",
    "- [ ] Have I **executed sufficient web searches** for the main question and each sub-question?\n",
    "- [ ] Have I combined web search results with internal knowledge?\n",
    "- [ ] **Have I ensured that the \"Detailed Answer\" is in-depth and comprehensive, covering multiple aspects and perspectives?**\n",
    "- [ ] **Have I included all references with plain text URLs in a \"References\" section placed just before the follow-up questions?**\n",
    "- [ ] **Did I aim to include more than three sources to enrich the response?**\n",
    "- [ ] **Did I avoid placing sources or links under each point or within the main response?**\n",
    "- [ ] **Did I structure the response with a \"Quick Answer\" and a \"Detailed Answer\", even for simple questions?**\n",
    "- [ ] Is the response organized for easy reading?\n",
    "- [ ] Did I include **3-5 follow-up questions** after the \"References\" section?\n",
    "- [ ] **Have I considered previous interactions** for personalization?\n",
    "- [ ] Have I stated the number of sources used in the \"References\" section?\n",
    "- [ ] Have I presented balanced perspectives?\n",
    "- [ ] Is the response compliant with ethical guidelines?\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Ollama Docker GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n",
    "    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n",
    "    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n",
    "    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "\n",
    "    \n",
    "sudo apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt-get install -y nvidia-container-toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo nvidia-ctk runtime configure --runtime=docker\n",
    "sudo systemctl restart docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Perplexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Perplexica'...\n",
      "remote: Enumerating objects: 2784, done.\u001b[K\n",
      "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 2784 (delta 108), reused 75 (delta 75), pack-reused 2644 (from 3)\u001b[K\n",
      "Receiving objects: 100% (2784/2784), 16.57 MiB | 52.38 MiB/s, done.\n",
      "Resolving deltas: 100% (1886/1886), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ItzCrazyKns/Perplexica.git ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexica url:  \n",
    "http://{server_ip}:3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Ollama Ubuntu Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo systemctl daemon-reload\n",
    "sudo systemctl enable ollama\n",
    "sudo systemctl start ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano /etc/systemd/system/ollama.service\n",
    "\n",
    "Environment=\"OLLAMA_HOST=0.0.0.0\"\n",
    "# to run llms forever until 2317 XD:\n",
    "Environment=\"OLLAMA_KEEP_ALIVE=-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama run llama3.3:70b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install deepseek-ai/DeepSeek-R1-Distill-Qwen-14B on Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo docker exec -it ollama /bin/bash\n",
    "ollama pull deepseek-r1:14b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run deepseek-r1:14b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install open-webui for Ollama (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install morphic instead of Perplexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'morphic'...\n",
      "remote: Enumerating objects: 3462, done.\u001b[K\n",
      "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
      "Receiving objects:  14% (485/3462)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Total 3462 (delta 100), reused 81 (delta 77), pack-reused 3292 (from 3)\u001b[K\n",
      "Receiving objects: 100% (3462/3462), 5.48 MiB | 26.08 MiB/s, done.\n",
      "Resolving deltas: 100% (2202/2202), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/amirh-far/morphic.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install Nginx for reverse proxing an api endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update && sudo apt install nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change nginx conf file:\n",
    "\n",
    "sudo nano /etc/nginx/sites-available/default\n",
    "\n",
    "### to this:\n",
    "server {\n",
    "    listen 11434;\n",
    "    server_name 0.0.0.0;\n",
    "\n",
    "    location /api/completions {\n",
    "        proxy_pass http://127.0.0.1:5000;  # Redirect to your Python server\n",
    "        proxy_set_header Host $host;\n",
    "        proxy_set_header X-Real-IP $remote_addr;\n",
    "        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "        proxy_set_header X-Forwarded-Proto $scheme;\n",
    "    }\n",
    "\n",
    "    location / {\n",
    "        proxy_pass http://127.0.0.1:11434;  # Forward all other requests to Ollama\n",
    "        proxy_set_header Host $host;\n",
    "        proxy_set_header X-Real-IP $remote_addr;\n",
    "        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "        proxy_set_header X-Forwarded-Proto $scheme;\n",
    "    }\n",
    "}\n",
    "\n",
    "### restart nginx\n",
    "sudo systemctl restart nginx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
